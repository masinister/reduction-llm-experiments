[server]
vllm_url = http://localhost:8000/v1
# model = openai/gpt-oss-20b
model = Qwen/Qwen3-0.6B
dtype = auto
gpu_memory_utilization = 0.8
tensor_parallel_size = 1
startup_timeout = 180

[inference]
# Default inference parameters
temperature = 0.3
max_tokens = 4096
max_context = 8192
repetition_penalty = 1.1

[chunking]
# Text chunking settings for long contexts
chunk_size = 1024
overlap = 128

[debug]
# Debug mode - pretty-print prompts and model responses
enabled = true
