[model]
use_toy_model = true
# Model to use for production 
model_id = openai/gpt-oss-20b
# Model to use for local testing/development (RTX 2070 Super with 8GB VRAM)
toy_model_id = Qwen/Qwen3-0.6B
# Model data type
dtype = half

[inference]
# Default inference parameters
temperature = 0.7
top_p = 0.95
top_k = 50
max_tokens = 2048

[system]
# Default system prompt
system_prompt = You are a helpful AI assistant.

[hardware]
# GPU configuration
# For single GPU (local dev): tensor_parallel_size = 1
# For multi-GPU (SLURM A100/H200): tensor_parallel_size = 2, 4, 8, ...
tensor_parallel_size = 1

# GPU memory utilization (0.0-1.0)
gpu_memory_utilization = 0.8

# Maximum model sequence length (input + output)
max_model_len = 8192

[compilation]
# vLLM cudagraph mode, one of: NONE, PIECEWISE, FULL, FULL_DECODE_ONLY, FULL_AND_PIECEWISE
cudagraph_mode = PIECEWISE

[context_budget]
# Context window budget allocation as percentages (must sum to ~1.0)
# System prompt allocation
system_pct = 0.05
# Running summary allocation
summary_pct = 0.10
# User prompt/chunk allocation
prompt_pct = 0.35
# Model output allocation
output_pct = 0.50
# Safety margin in tokens
safety_margin_tokens = 32
