[server]
vllm_url = http://localhost:8000/v1
model = Qwen/Qwen3-0.6B
dtype = half
gpu_memory_utilization = 0.8
tensor_parallel_size = 1
startup_timeout = 180

[inference]
# Default inference parameters
temperature = 0.4
max_tokens = 1024
max_context = 8192

[chunking]
# Text chunking settings for long contexts
chunk_size = 2048
overlap = 64
