[server]
vllm_url = http://localhost:8000/v1
model = openai/gpt-oss-20b
dtype = auto
gpu_memory_utilization = 0.8
tensor_parallel_size = 4
startup_timeout = 180

[inference]
# Default inference parameters
temperature = 0.4
max_tokens = 4096
max_context = 8192

[chunking]
# Text chunking settings for long contexts
chunk_size = 2048
overlap = 128
