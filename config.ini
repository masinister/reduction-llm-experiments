# Model Configuration
# 
# This file contains default settings for model initialization.
# You can override any of these by passing kwargs to Model() or load_from_config()

[model]
# Model to use for production (RTX 2070 Super has 8GB VRAM)
model_id = openai/gpt-oss-20b
# Model to use for local testing/development (vLLM requires decoder-only models)
toy_model_id = Qwen/Qwen3-0.6B

[inference]
# Default inference parameters
temperature = 0.7
top_p = 0.95
top_k = 50
max_tokens = 8096

[system]
# Default system prompt
system_prompt = You are a helpful AI assistant.

[hardware]
# GPU configuration
# For single GPU (local dev): tensor_parallel_size = 1
# For multi-GPU (SLURM A100/H200): tensor_parallel_size = 2, 4, 8, ...
tensor_parallel_size = 4

# GPU memory utilization (0.0-1.0)
gpu_memory_utilization = 0.8

# Maximum model sequence length (input + output)
max_model_len = 20480
