[server]
vllm_url = http://localhost:8000/v1
model = openai/gpt-oss-20b
dtype = auto
gpu_memory_utilization = 0.8
tensor_parallel_size = 4
startup_timeout = 180

[inference]
# Default inference parameters
temperature = 0.4
max_tokens = 8192
max_context = 16384

[chunking]
# Text chunking settings for long contexts
chunk_size = 4096
overlap = 128

[debug]
# Debug mode - pretty-print prompts and model responses
enabled = true
