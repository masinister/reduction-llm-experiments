[model]
use_toy_model = true
# Model to use for production 
model_id = openai/gpt-oss-20b
# Model to use for local testing/development (RTX 2070 Super with 8GB VRAM)
toy_model_id = Qwen/Qwen3-0.6B
# Model data type
dtype = half

[inference]
# Default inference parameters
temperature = 0.7
top_p = 0.95
top_k = 50
max_tokens = 1024

[debug]
# Enable debug mode for verbose logging and pretty-printing
debug = true

[hardware]
# GPU configuration
# For single GPU (local dev): tensor_parallel_size = 1
# For multi-GPU (SLURM A100/H200): tensor_parallel_size = 2, 4, 8, ...
tensor_parallel_size = 1

# GPU memory utilization (0.0-1.0)
gpu_memory_utilization = 0.8

# Maximum model sequence length (input + output)
max_model_len = 4096

[compilation]
# vLLM cudagraph mode, one of: NONE, PIECEWISE, FULL, FULL_DECODE_ONLY, FULL_AND_PIECEWISE
cudagraph_mode = PIECEWISE

[chunking]
# Text chunking settings for long contexts
chunk_size = 1024
overlap = 64
